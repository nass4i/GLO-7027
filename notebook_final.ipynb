{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.stats import variation\n",
    "import random\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation MICE et KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Assemblage du dataset à impute\n",
    "CES19_data = pd.read_csv('C:/Users/nnass/Documents/data_GLO-7027/CES19.csv')\n",
    "\n",
    "CES19_converted = CES19_data.infer_objects()\n",
    "\n",
    "data_ces_only = CES19_converted.copy()\n",
    "data_ces_only = data_ces_only.loc[:,~data_ces_only.columns.str.startswith('pes19')].copy()\n",
    "\n",
    "list_todel = ['cps19_votechoice', 'cps19_votechoice_pr', 'cps19_vote_unlikely', 'cps19_vote_unlike_pr', 'cps19_v_advance',\n",
    "              'cps19_votechoice_7_TEXT', 'cps19_votechoice_pr_7_TEXT', 'cps19_vote_unlikely_7_TEXT', 'cps19_vote_unlike_pr_7_TEXT',\n",
    "              'cps19_v_advance_7_TEXT', 'cps19_vote_lean', 'cps19_vote_lean_7_TEXT', 'cps19_vote_lean_pr', 'cps19_vote_lean_pr_7_TEXT',\n",
    "              'cps19_2nd_choice', 'cps19_2nd_choice_7_TEXT', 'cps19_2nd_choice_pr', 'cps19_2nd_choice_pr_7_TEXT', 'cps19_not_vote_for_1',\n",
    "              'cps19_not_vote_for_2', 'cps19_not_vote_for_3', 'cps19_not_vote_for_4', 'cps19_not_vote_for_5', 'cps19_not_vote_for_6',\n",
    "              'cps19_not_vote_for_7', 'cps19_not_vote_for_8', 'cps19_not_vote_for_9', 'cps19_not_vote_for_7_TEXT']\n",
    "\n",
    "# Une variété de colonnes de metadata sont aussi retirées\n",
    "list_meta = ['cps19_weight_general_all', 'cps19_weight_general_restricted',\n",
    "             'cps19_duplicates_flag', 'get_news', 'get_more_naming',\n",
    "             'get_not_vote_for', 'get_party_issue_handling', 'get_imp_loc_iss',\n",
    "             'get_outcome', 'justice_law', 'justice_law_fr', 'lr_scale_order',\n",
    "             'ethnicity_intro', 'ethnicity_intro_fr', 'premier', 'province_fr',\n",
    "             'pid_en', 'pid_party_en', 'pid_party_fr', 'confidence_institutions_word',\n",
    "             'confidence_institutions_word_fr', 'govt_programs_word',\n",
    "             'govt_programs_word_fr', 'split_taxes', 'split_senate', 'split_trade',\n",
    "             'split_lifesat', 'split_responsibility', 'split_sexism',\n",
    "             'split_abortion', 'split_getahead', 'split_att_div', 'split_govt_eff',\n",
    "             'split_medical', 'split_ties', 'split_health_followups',\n",
    "             'split_gender_id', 'split_big5', 'split_hatespeech', 'split_vol_assoc',\n",
    "             'notvote_split', 'splitsample', 'constituencyname', 'pes10_socnet3']\n",
    "             # La dernière colonne est une question du pes\n",
    "\n",
    "# Puis, les cinq colonnes d'intentions de vote sont combinées en une seule (qui gardera le nom vote_choice):\n",
    "votes = data_ces_only[[\"cps19_votechoice\", \"cps19_votechoice_pr\", \"cps19_vote_unlikely\", \"cps19_vote_unlike_pr\", \"cps19_v_advance\"]]\n",
    "votes = votes.fillna('').sum(axis=1)\n",
    "data_ces_only['cps19_votechoice'] = votes\n",
    "\n",
    "# Certaines rangées ne contenaient aucune intention de vote. La chaîne vide est\n",
    "# replacée par un NaN\n",
    "data_ces_only['cps19_votechoice'] = np.where(data_ces_only['cps19_votechoice'] == '', np.NaN,\n",
    "                                             data_ces_only['cps19_votechoice'])\n",
    "\n",
    "# Les intentions de vote que l'on cherche à prédire sont aussi importées, de façon à pouvoir être retirées\n",
    "to_pred = pd.read_table('C:/Users/nnass/Documents/data_GLO-7027/exemple_GLO.txt',\n",
    "                        header=None, index_col=0)\n",
    "# On se retrouve bien avec un NaN en intention de vote pour chacune de ces rangées, donc l'étape précédente semble\n",
    "# être correcte\n",
    "\n",
    "# Les colonnes inutiles sont retirées et les intentions de vote sont préalablement séparées\n",
    "data_todel = list_todel + list_meta\n",
    "vote_int = data_ces_only[list_todel].copy()\n",
    "data_ready = data_ces_only.drop(labels=data_todel, axis=1)\n",
    "# Les IDs et les dates sont retirés pour l'instant\n",
    "data_ready = data_ready.drop(labels=['Unnamed: 0', 'cps19_consent',\n",
    "                                     'cps19_StartDate', 'cps19_EndDate',\n",
    "                                     'cps19_ResponseId'], axis=1)\n",
    "\n",
    "# Puis, les données numériques et textuelles sont séparées\n",
    "num_cols = np.where(data_ready.dtypes != 'object')\n",
    "data_num = data_ready.iloc[:, np.r_[num_cols]]\n",
    "\n",
    "text_cols = np.where(data_ready.dtypes == 'object')\n",
    "data_text = data_ready.iloc[:, np.r_[text_cols]]\n",
    "\n",
    "# Des données numériques qui ne sont pas utiles sont retirées\n",
    "num_not = ['cps19_current_date', 'cps19_current_date_string',\n",
    "           'cps19_Q_TotalDuration', 'cps19_data_quality', 'cps19_inattentive',\n",
    "           'constituencynumber']\n",
    "data_num = data_num.drop(labels=num_not, axis=1)\n",
    "\n",
    "# Dataframe de one-hot vectors pour les attributs encodés ainsi:\n",
    "data_text_NaN = data_text.fillna('NaN')\n",
    "uniques = data_text_NaN.nunique()\n",
    "\n",
    "one_hot = np.where(uniques == 2)\n",
    "one_hot_df = data_text_NaN.iloc[:, np.r_[one_hot]]\n",
    "\n",
    "one_hot_df = one_hot_df.drop(labels=['cps19_Q_Language'], axis=1)\n",
    "one_hot_df.iloc[:, :] = np.where(one_hot_df.iloc[:, :] == 'NaN', 0, 1)\n",
    "\n",
    "# On peut ensuite assembler le dataset utilisé pour l'imputation. Afin d'accélérer le processus et de limiter le bruit,\n",
    "# seuls les attributs qui seront utilisés dans au moins une tentative de modèle sont conservés\n",
    "num_impute = data_num[['cps19_interest_gen_1', 'cps19_interest_elxn_1', 'cps19_age', 'cps19_party_rating_23',\n",
    "                       'cps19_party_rating_24', 'cps19_party_rating_25', 'cps19_party_rating_26',\n",
    "                       'cps19_party_rating_27', 'cps19_party_rating_28', 'cps19_lead_rating_23',\n",
    "                       'cps19_lead_rating_24', 'cps19_lead_rating_25', 'cps19_lead_rating_26',\n",
    "                       'cps19_lead_rating_27', 'cps19_lead_rating_28', 'cps19_cand_rating_23',\n",
    "                       'cps19_cand_rating_24', 'cps19_cand_rating_25',\n",
    "                       'cps19_cand_rating_27', 'cps19_cand_rating_28']].copy()\n",
    "\n",
    "cats_tokeep = data_text[['cps19_imp_iss_party', 'cps19_fed_id', 'cps19_vote_2015', 'cps19_prov_id', 'cps19_fed_gov_sat',\n",
    "                         'cps19_issue_handle_8', 'cps19_issue_handle_2']].copy()\n",
    "\n",
    "cats_tokeep = pd.get_dummies(cats_tokeep)\n",
    "\n",
    "mask_onehot = one_hot_df.columns.str.contains(r'cps19_lead_*')\n",
    "one_hot_sel = one_hot_df.loc[:, mask_onehot]\n",
    "one_hot_keep = one_hot_df[['cps19_ethnicity_38', 'cps19_language_69', 'cps19_party_member_38']]\n",
    "one_hot_keep = pd.concat([one_hot_keep, one_hot_sel], axis=1)\n",
    "\n",
    "# Dataset final\n",
    "data_selected = pd.concat([one_hot_keep, num_impute, cats_tokeep], axis=1)\n",
    "\n",
    "# Les données numériques sont normalisées min-max\n",
    "scaler = MinMaxScaler()\n",
    "data_selected = pd.DataFrame(scaler.fit_transform(data_selected), columns=data_selected.columns,\n",
    "                             index=data_selected.index)\n",
    "\n",
    "# Toutes les intentions de vote sont ajoutées et le dataset est subdivisé\n",
    "data_selected = pd.concat([data_selected, vote_int['cps19_votechoice']], axis=1)\n",
    "\n",
    "features_data = data_selected.drop(labels=to_pred.index, axis=0)\n",
    "features_data = features_data.dropna(subset='cps19_votechoice')\n",
    "labels_train = features_data['cps19_votechoice']\n",
    "features_data = features_data.drop(labels='cps19_votechoice', axis=1)\n",
    "\n",
    "to_pred_feats = data_selected.iloc[np.r_[to_pred.index], :]\n",
    "to_pred_feats = data_selected.drop(labels='cps19_votechoice', axis=1)\n",
    "\n",
    "# L'ensemble de données \"d'entraînement\" est divisé en un ensemble de training (75%) et de test (25%) afin de pouvoir\n",
    "# évaluer la justesse de l'imputation des données manquantes\n",
    "train_feats, test_feats, train_labels, test_labels = train_test_split(features_data, labels_train,\n",
    "                                                                      test_size=0.25, random_state=42)\n",
    "\n",
    "# Les ensembles de données sont sauvegardés\n",
    "train_feats.to_csv('C:/Users/nnass/Documents/data_GLO-7027/train_feats_impute.csv')\n",
    "train_labels.to_csv('C:/Users/nnass/Documents/data_GLO-7027/train_labels_impute.csv')\n",
    "\n",
    "test_feats.to_csv('C:/Users/nnass/Documents/data_GLO-7027/test_feats_impute.csv')\n",
    "test_labels.to_csv('C:/Users/nnass/Documents/data_GLO-7027/test_labels_impute.csv')\n",
    "\n",
    "to_pred_feats.to_csv('C:/Users/nnass/Documents/data_GLO-7027/to_pred_selected.csv')\n",
    "\n",
    "# 2) Imputation par MICE\n",
    "# D'abord, l'imputer est fit sur l'ensemble train_feats\n",
    "lr = LinearRegression()\n",
    "imp = IterativeImputer(estimator=lr, missing_values=np.nan, max_iter=40, verbose=2, imputation_order='roman',\n",
    "                       random_state=0)\n",
    "\n",
    "train_feats_fit = imp.fit_transform(train_feats)\n",
    "train_feats_fit = pd.DataFrame(train_feats_fit, columns=train_feats.columns, index=train_feats.index)\n",
    "\n",
    "train_feats_fit.to_csv('C:/Users/nnass/Documents/data_GLO-7027/train_feats_MICE.csv')\n",
    "\n",
    "# Ensuite, il est utilisé pour transformer test_feats\n",
    "test_feats_MICE = imp.transform(test_feats)\n",
    "test_feats_MICE = pd.DataFrame(test_feats_MICE, columns=test_feats.columns, index=test_feats.index)\n",
    "\n",
    "test_feats_MICE.to_csv('C:/Users/nnass/Documents/data_GLO-7027/test_feats_MICE.csv')\n",
    "\n",
    "# Finalement, les données pour lesquelles on doit faire la prédiction sont aussi transformées\n",
    "to_pred_feats_MICE = imp.transform(to_pred_feats)\n",
    "to_pred_feats_MICE = pd.DataFrame(to_pred_feats_MICE, columns=to_pred_feats.columns, index=to_pred_feats.index)\n",
    "\n",
    "to_pred_feats_MICE.to_csv('C:/Users/nnass/Documents/data_GLO-7027/topred_feats_MICE.csv')\n",
    "\n",
    "# 3) Imputation par KNN\n",
    "# Une boucle est d'abord nécessaire pour identifier le paramètre k optimal\n",
    "df_model = pd.DataFrame(columns=['k', 'f1_weighted', 'balanced_accuracy'])\n",
    "results = df_model.copy()\n",
    "strategies = [5, 10, 15, 20, 25, 35, 50, 75, 100, 150, 500]  # k tentés\n",
    "\n",
    "# Paramètres\n",
    "model = DecisionTreeClassifier(class_weight='balanced')\n",
    "cv = KFold(n_splits=10)\n",
    "scoring = ['f1_macro', 'balanced_accuracy']\n",
    "\n",
    "# Scaling des données numériques\n",
    "scaler = MinMaxScaler()\n",
    "train_feats = pd.DataFrame(scaler.fit_transform(train_feats), columns=train_feats.columns)\n",
    "\n",
    "for k in strategies:\n",
    "    # Création du pipeline\n",
    "    pipeline = Pipeline(steps=[('KNN', KNNImputer(n_neighbors=k)),\n",
    "                               ('Tree', model)])\n",
    "\n",
    "    # Évaluation\n",
    "    scores = cross_validate(pipeline, train_feats, train_labels, scoring=scoring,\n",
    "                            cv=cv, error_score='raise', verbose=2)\n",
    "\n",
    "    F1 = scores['test_f1_macro']\n",
    "    accuracy = scores['test_balanced_accuracy']\n",
    "\n",
    "    # Sauvegarde des résultats\n",
    "    res_CV = df_model.copy()\n",
    "    res_CV['f1_macro'] = F1\n",
    "    res_CV['balanced_accuracy'] = accuracy\n",
    "    res_CV['k'] = k\n",
    "\n",
    "    results = pd.concat([results, res_CV]).reset_index(drop=True)\n",
    "# Figure pour représenter les résultats\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "med_f1 = results[results['k'] == 50]['f1_macro'].median()\n",
    "med_accuracy = results[results['k'] == 50]['balanced_accuracy'].median()\n",
    "\n",
    "sns.boxplot(x='k', y='f1_macro', data=results, ax=axs[0],\n",
    "            palette='plasma')\n",
    "axs[0].axhline(y=med_f1, c='k', linestyle='--', linewidth=2.5)\n",
    "\n",
    "sns.boxplot(x='k', y='balanced_accuracy', data=results, ax=axs[1],\n",
    "            palette='plasma')\n",
    "axs[1].axhline(y=med_accuracy, c='k', linestyle='--', linewidth=2.5)\n",
    "\n",
    "# Axes\n",
    "axs[0].set_xlabel('')\n",
    "axs[1].set_xlabel('Nombre de voisins k', fontsize=14)\n",
    "\n",
    "axs[0].set_ylabel('Score F1 macro', fontsize=14)\n",
    "axs[1].set_ylabel('Exactitude équilibrée', fontsize=14)\n",
    "\n",
    "Fig_k = plt.gcf()\n",
    "Fig_k.savefig('C:/Users/nnass/Documents/data_GLO-7027/Figure_KNN_k.pdf', bbox_inches='tight')\n",
    "\n",
    "# Un k de 50 semble donner les meilleurs résultats (à confirmer avec F1-macro)\n",
    "# On l'utilise donc pour l'imputation\n",
    "\n",
    "# D'abord sur le set \"d'entraînement\"\n",
    "imputer = KNNImputer(n_neighbors=50)\n",
    "train_feats_KNN = imputer.fit_transform(train_feats)\n",
    "train_feats_KNN = pd.DataFrame(train_feats_KNN, columns=train_feats.columns, index=train_feats.index)\n",
    "\n",
    "train_feats_KNN.to_csv('C:/Users/nnass/Documents/data_GLO-7027/train_feats_KNN.csv')\n",
    "\n",
    "# Puis pour le set de test\n",
    "test_feats_KNN = imputer.transform(test_feats)\n",
    "test_feats_KNN = pd.DataFrame(test_feats_KNN, columns=test_feats.columns, index=test_feats.index)\n",
    "\n",
    "test_feats_KNN.to_csv('C:/Users/nnass/Documents/data_GLO-7027/test_feats_KNN.csv')\n",
    "\n",
    "# Et finalement, pour les données sur lesquelles on doit faire nos prédictions\n",
    "to_pred_feats_KNN = imputer.transform(to_pred_feats)\n",
    "to_pred_feats_KNN = pd.DataFrame(to_pred_feats_KNN, columns=to_pred_feats.columns, index=to_pred_feats.index)\n",
    "\n",
    "to_pred_feats_KNN.to_csv('C:/Users/nnass/Documents/data_GLO-7027/topred_feats_KNN.csv')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
